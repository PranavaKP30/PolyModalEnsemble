# Amazon Reviews 5-Class Rating Prediction Experiment
## Comprehensive Modality-Aware Adaptive Bagging Ensemble Evaluation Framework

---

## ğŸ“Š Dataset Information & Machine Learning Task

### **Dataset Overview**
- **Source**: Stanford SNAP Amazon Product Reviews (Electronics Category)  
- **Task Type**: 5-class ordinal rating prediction (1â˜… to 5â˜…)
- **Dataset Size**: 200,000 training samples, 50,000 test samples
- **Modalities**: Text reviews + Product metadata
- **Target Variable**: Customer rating scores (1, 2, 3, 4, 5 stars)

### **Data Characteristics**
| Component | Description | Dimensions | Details |
|-----------|-------------|------------|---------|
| **Text Features** | TF-IDF vectorized review content | 1,000D | Customer review text + summary |
| **Metadata Features** | Product & review statistics | 14D | Price, category, helpfulness, etc. |
| **Labels** | Rating predictions | 5 classes | Ordinal classification (1-5 stars) |
| **Data Split** | Stratified sampling | 80/20 | Maintains class distribution |

### **Machine Learning Challenge**
**Objective**: Predict customer satisfaction ratings from multimodal review data

**Key Challenges**:
- **Multimodal Integration**: Combining textual sentiment with numerical metadata
- **Class Imbalance**: Uneven distribution across 5 rating categories  
- **Feature Dimensionality**: High-dimensional text features (1000D) vs compact metadata (14D)
- **Ordinal Relationships**: Preserving rating order (1 < 2 < 3 < 4 < 5)
- **Real-world Noise**: Authentic customer reviews with inconsistent language

---

## ğŸ§ª Scientific Hypothesis Testing Framework

### **Primary Research Hypothesis**
> *"A modality-aware adaptive bagging ensemble with strategic dropout mechanisms will achieve superior performance on Amazon Reviews rating prediction compared to traditional single-modality and early-fusion approaches, while maintaining robustness to missing modality scenarios."*

### **Experimental Design Principles**

#### **1. Hypothesis Validation Components**
- **âœ… Testable Hypothesis**: Clear, measurable prediction about ensemble performance
- **âœ… Variable Manipulation**: Independent variables (model architecture, modalities) vs dependent variables (accuracy, MAE)
- **âœ… Control Variables**: Fixed random seeds (42), consistent train/test splits, standardized evaluation metrics
- **âœ… Control Groups**: 30+ baseline models serve as comprehensive control conditions
- **âœ… Random Assignment**: Stratified sampling ensures unbiased data distribution
- **âœ… Dependent Variable Measurement**: Multiple metrics (Accuracy, Star MAE, Â±1 Star Accuracy, F1, RÂ²)
- **âœ… Statistical Analysis**: Cross-validation, significance testing, confidence intervals
- **âœ… Validity & Reliability**: Multi-seed experiments, robustness testing, ablation studies

#### **2. Scientific Rigor Standards**
- **Reproducibility**: Fixed random seeds and deterministic procedures
- **Statistical Significance**: 5-fold cross-validation with confidence intervals  
- **Multiple Comparisons**: Bonferroni correction for baseline comparisons
- **Effect Size**: Cohen's d for practical significance assessment
- **Robustness Validation**: Performance under noise, missing data, class imbalance

---

## ğŸ—ï¸ Experimental Pipeline Architecture

The experiment follows a rigorous 6-phase scientific pipeline designed for comprehensive model evaluation:

### **Phase 1: Data Preparation & Integration**
**Purpose**: Load, preprocess, and prepare multimodal Amazon Reviews data

**Components**:
- **Dataset Loading**: Amazon Reviews from preprocessed NumPy arrays
- **Feature Engineering**: 
  - Text: TF-IDF vectorization (1000 dimensions)
  - Metadata: Numerical features (14 dimensions) 
- **Data Sampling**: Configurable subset selection for quick/full modes
- **Stratified Splitting**: Maintains class distribution across train/test
- **Normalization**: StandardScaler for metadata features
- **Modality Organization**: Separate text, metadata, and combined feature matrices

**Key Features**:
```python
# Text features: Customer reviews + summaries (TF-IDF)
text_features: (n_samples, 1000)  # High-dimensional sparse features

# Metadata: Product characteristics + review statistics  
metadata_features: (n_samples, 14)  # Compact numerical features

# Labels: Rating prediction target
labels: (n_samples,)  # Values: 0, 1, 2, 3, 4 (representing 1-5 stars)
```

---

### **Phase 2: Comprehensive Baseline Evaluation**
**Purpose**: Establish performance benchmarks across 30+ traditional ML models

**Model Categories**:

#### **Category 1: Single Modality Models (6 models)**
- **Text-Only**: Random Forest, Logistic Regression, MLP on text features
- **Metadata-Only**: Random Forest, Logistic Regression, Gradient Boosting on metadata

#### **Category 2: Early Fusion Models (8 models)**  
- **Combined Features**: Concatenated text + metadata features
- **Algorithms**: Random Forest, Logistic Regression, MLP, Gradient Boosting, SVM, k-NN, Naive Bayes, Decision Tree

#### **Category 3: Advanced Ensemble Methods (6 models)**
- **XGBoost**: Gradient boosting with advanced regularization
- **Extra Trees**: Extremely randomized trees ensemble
- **AdaBoost**: Adaptive boosting with weak learners
- **Voting Ensemble**: Hard/soft voting across multiple algorithms
- **Stacking Ensemble**: Meta-learner combining base predictions
- **Bagging**: Bootstrap aggregating with variance reduction

#### **Category 4: Multimodal Fusion Models (6 models)**
- **Late Fusion Voting**: Separate modality models + majority voting
- **Late Fusion Stacking**: Meta-learner combining modality predictions  
- **Attention Fusion**: Attention weights for modality importance
- **Tensor Fusion**: Tensor product of modality representations
- **Multiplicative Fusion**: Element-wise modality interactions
- **Wide & Deep**: Linear wide component + deep neural component

#### **Category 5: State-of-the-Art Models (4 models)**
- **Stacking Ensemble**: Advanced meta-learning architectures
- **Deep Neural Network**: Multi-layer perceptron with dropout
- **Wide Neural Network**: Broad shallow architecture  
- **Regularized Neural Network**: L1/L2 regularization with early stopping

**Evaluation Protocol**:
- Consistent train/test splits across all models
- Identical feature preprocessing pipeline
- Comprehensive metrics: Accuracy, Star MAE, Â±1 Star Accuracy, F1, RÂ², training/inference time
- Error handling for failed models with detailed logging

---

### **Phase 3: MainModel Hyperparameter Optimization & Evaluation**

#### **Novel MainModel Architecture**
The **Modality-Aware Adaptive Bagging Ensemble** introduces several innovative components:

##### **ğŸ”¬ Novel Feature 1: Modality-Aware Dropout Strategy**
- **Linear Dropout**: Gradual modality availability reduction (100% â†’ 0%)
- **Exponential Dropout**: Rapid early dropout with gradual tapering  
- **Adaptive Dropout**: Dynamic adjustment based on modality importance

##### **ğŸ”¬ Novel Feature 2: Strategic Ensemble Bagging**
- **Heterogeneous Base Learners**: Multiple algorithm types per bag
- **Modality-Specific Specialization**: Different models for text vs metadata
- **Adaptive Bag Weighting**: Performance-based ensemble weights

##### **ğŸ”¬ Novel Feature 3: Meta-Learning Integration**
- **Cross-Modal Attention**: Transformer-based modality fusion
- **Uncertainty Estimation**: Confidence-aware predictions
- **Dynamic Architecture**: Runtime adaptation to data characteristics

#### **Hyperparameter Optimization Process**

**Search Space Definition**:
```python
# Quick Mode (Development)
param_grid_quick = {
    'n_bags': [3, 5],                           # Ensemble size
    'dropout_strategy': ['linear', 'exponential'], # Dropout type  
    'epochs': [5, 10, 15],                      # Training iterations
    'batch_size': [256],                        # Fixed for consistency
    'random_state': [42 + trial_id]             # Unique per trial
}
# Total combinations: 12 trials

# Full Mode (Production)  
param_grid_full = {
    'n_bags': [3, 5, 10],                       # Extended ensemble sizes
    'dropout_strategy': ['linear', 'exponential', 'adaptive'], # All strategies
    'epochs': [5, 10, 15],                      # Same epoch range
    'batch_size': [256],                        # Consistent batch size
    'random_state': [42 + trial_id]             # Unique seeds
}
# Total combinations: 27 trials (Quick) / 54 trials (Full)
```

**Optimization Protocol**:
1. **Grid Search**: Exhaustive evaluation of hyperparameter combinations
2. **Unique Random Seeds**: Each trial uses different random state for diversity
3. **MAE Optimization**: Star Mean Absolute Error as primary optimization metric
4. **Early Stopping**: Prevent overfitting with validation monitoring
5. **Best Parameter Selection**: Lowest MAE configuration chosen

**MainModel Evaluation Tests**:
- **Simple Test**: Default hyperparameters for baseline performance
- **Optimized Test**: Best hyperparameters from optimization process
- **Performance Comparison**: Simple vs Optimized to measure improvement
- **Novel Feature Validation**: Ablation studies on modality-aware components

---

### **Phase 4: Advanced Testing & Analysis**

#### **Cross-Validation Analysis**
**Purpose**: Statistical validation of model performance and stability

**Protocol**:
- **5-Fold Stratified CV**: Maintains class distribution in each fold
- **Selected Models**: 5 representative models (computational efficiency)
  - RandomForest_Text (single modality baseline)
  - RandomForest_Combined (early fusion baseline)  
  - XGBoost_Combined (advanced baseline)
  - VotingEnsemble (ensemble baseline)
  - MainModel (novel architecture)

**Statistical Analysis**:
- **Mean Â± Standard Deviation**: Central tendency and variability
- **95% Confidence Intervals**: Statistical reliability bounds
- **Stability Assessment**: Coefficient of variation across folds
- **Significance Testing**: Paired t-tests between models

#### **Robustness Testing**
**Purpose**: Evaluate model performance under adverse conditions

**Test Categories**:

##### **1. Noise Resilience Testing**
- **Gaussian Noise**: Add random noise to numerical features
- **Text Corruption**: Random character/word substitutions  
- **Feature Dropout**: Random feature masking
- **Noise Levels**: 5%, 10%, 15%, 20% corruption rates

##### **2. Missing Data Handling**
- **Random Missing**: Uniform random feature removal
- **Systematic Missing**: Structured missing patterns
- **Modality-Specific**: Complete text or metadata absence
- **Missing Rates**: 10%, 25%, 50% missing data scenarios

##### **3. Class Imbalance Sensitivity**
- **Imbalance Simulation**: Artificially skew class distributions
- **Minority Class Reduction**: Reduce rare rating occurrences
- **Majority Class Oversampling**: Increase common ratings
- **Imbalance Ratios**: 2:1, 5:1, 10:1 majority to minority

##### **4. Sample Size Sensitivity**
- **Learning Curve Analysis**: Performance vs training set size
- **Sample Sizes**: 100, 250, 500, 1000, 2000, Full dataset
- **Efficiency Measurement**: Performance improvement per additional sample
- **Overfitting Detection**: Training vs validation performance gaps

#### **Interpretability Analysis**
**Purpose**: Understand model decision-making processes and feature importance

**Analysis Components**:

##### **1. Feature Importance Analysis**
- **Model-Agnostic**: SHAP values and permutation importance
- **Model-Specific**: Built-in feature importance scores
- **Modality Comparison**: Text vs metadata feature contributions
- **Top Features**: Most predictive features per modality and overall

##### **2. Model Insights Extraction**
- **Decision Boundaries**: Visualization of rating prediction regions
- **Attention Weights**: Transformer attention patterns for text
- **Interaction Effects**: Feature combinations and synergies
- **Prediction Confidence**: Uncertainty quantification

##### **3. Error Pattern Analysis**
- **Confusion Matrix**: Detailed error breakdown by rating class
- **Error Categories**: Systematic vs random prediction errors
- **Difficult Cases**: Samples with consistently poor predictions
- **Rating-Specific**: Per-class error patterns and common mistakes

#### **Comprehensive Ablation Studies**
**Purpose**: Systematic evaluation of component contributions

**Ablation Categories**:

##### **1. Basic Modality Ablation**
- **Text-Only**: Performance using only review text features
- **Metadata-Only**: Performance using only product metadata  
- **Both Modalities**: Full multimodal performance
- **Multimodal Benefit**: Quantified improvement from combining modalities

##### **2. Text Feature Subset Ablation**
- **Feature Quartiles**: Performance on 25%, 50%, 75%, 100% of text features
- **TF-IDF Components**: Document frequency vs term frequency importance
- **Vocabulary Size**: Impact of vocabulary truncation
- **Feature Selection**: Optimal text feature subset identification

##### **3. Metadata Feature Ablation**
- **Individual Feature Removal**: Performance drop per metadata feature
- **Feature Categories**: Product vs review metadata importance
- **Feature Correlation**: Redundant vs complementary metadata
- **Optimal Subset**: Minimal metadata for maximum performance

##### **4. Fusion Strategy Ablation**
- **Early Fusion**: Simple concatenation baseline
- **Late Fusion**: Separate model combination
- **Weighted Fusion**: Learned modality weights
- **Attention Fusion**: Transformer-based combination
- **Strategy Comparison**: Optimal fusion approach identification

##### **5. Architecture Component Ablation**
- **Dropout Strategy**: Linear vs exponential vs adaptive
- **Ensemble Size**: 1, 3, 5, 10 bags performance comparison
- **Base Learner Types**: Algorithm diversity impact
- **Meta-Learning**: With vs without meta-learner components

---

## ğŸ“ Results Organization & Analysis Framework

### **Comprehensive Results Structure**

Every experiment run creates a timestamped directory with complete experimental results:

```
results/run_YYYYMMDD_HHMMSS/
â”œâ”€â”€ ğŸ“‚ seed_1/                              # Individual seed results
â”‚   â”œâ”€â”€ ğŸ“‚ 01_baseline_models/              # Baseline evaluation results
â”‚   â”‚   â”œâ”€â”€ individual_results.json        # All model performances
â”‚   â”‚   â”œâ”€â”€ summary.json                   # Best model summary  
â”‚   â”‚   â”œâ”€â”€ performance_comparison.csv     # Tabular performance data
â”‚   â”‚   â”œâ”€â”€ detailed_report.txt           # Human-readable analysis
â”‚   â”‚   â””â”€â”€ ğŸ“‚ individual_predictions/     # Per-model prediction files
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ 02_hyperparameter_tuning/       # HP optimization results
â”‚   â”‚   â”œâ”€â”€ best_parameters.json          # Optimal hyperparameters
â”‚   â”‚   â”œâ”€â”€ trial_history.json            # All trial results
â”‚   â”‚   â”œâ”€â”€ optimization_summary.txt      # Optimization analysis
â”‚   â”‚   â””â”€â”€ ğŸ“‚ trial_predictions/         # Per-trial predictions
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ 03_main_model/                  # MainModel evaluation
â”‚   â”‚   â”œâ”€â”€ final_metrics.json            # Performance metrics
â”‚   â”‚   â”œâ”€â”€ model_configuration.json      # Model setup
â”‚   â”‚   â””â”€â”€ ğŸ“‚ test_predictions/          # Prediction files
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ 04_cross_validation/            # CV analysis results
â”‚   â”‚   â”œâ”€â”€ cv_statistics.json            # CV performance stats
â”‚   â”‚   â”œâ”€â”€ fold_results.json             # Per-fold results
â”‚   â”‚   â””â”€â”€ ğŸ“‚ fold_predictions/          # Per-fold predictions
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ 05_interpretability/            # Model interpretation
â”‚   â”‚   â”œâ”€â”€ feature_importance.json       # Feature rankings
â”‚   â”‚   â”œâ”€â”€ model_insights.json          # Decision insights
â”‚   â”‚   â””â”€â”€ error_analysis.json          # Error patterns
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ 06_ablation_studies/            # Component analysis
â”‚   â”‚   â”œâ”€â”€ modality_contributions.json   # Modality importance
â”‚   â”‚   â”œâ”€â”€ strategy_comparison.json      # Fusion strategies
â”‚   â”‚   â””â”€â”€ ablation_analysis.txt         # Comprehensive analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ 07_robustness_analysis/         # Robustness testing
â”‚   â”‚   â”œâ”€â”€ robustness_tests.json         # Test results
â”‚   â”‚   â””â”€â”€ robustness_report.txt         # Analysis summary
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ 08_visualizations/              # Performance plots & charts
â”‚   â”‚   â””â”€â”€ (various visualization files) # Generated plots
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ 09_raw_data/                    # Raw experimental data
â”‚       â”œâ”€â”€ complete_results.pkl          # Full results object
â”‚       â””â”€â”€ all_predictions.pkl          # All model predictions
â”‚
â”œâ”€â”€ ğŸ“‚ aggregate_results/                   # Multi-seed aggregation
â”‚   â”œâ”€â”€ cross_seed_summary.json           # Aggregate statistics
â”‚   â””â”€â”€ statistical_tests.json            # Significance tests
â”‚
â”œâ”€â”€ ğŸ“‚ experiment_summary/                 # Experiment overview
â”‚   â”œâ”€â”€ executive_summary.txt             # High-level findings
â”‚   â””â”€â”€ experiment_configuration.json     # Full configuration
â”‚
â”œâ”€â”€ ğŸ“‚ publication/                        # Publication-ready exports
â”‚   â”œâ”€â”€ performance_table.csv             # Results table
â”‚   â”œâ”€â”€ statistical_analysis.txt          # Statistical summary
â”‚   â”œâ”€â”€ experimental_setup.txt            # Methods description
â”‚   â””â”€â”€ key_findings.txt                  # Main conclusions
â”‚
â””â”€â”€ experiment_manifest.json               # Experiment metadata
```

### **Example Results Analysis**

#### **Performance Summary Table** (sample from actual run)
```csv
Model,Accuracy,Star_MAE,Close_Accuracy,Training_Time_s
metadata_lr,0.5900,0.7900,0.7800,0.01
combined_knn,0.5900,0.8000,0.7800,0.00
extra_trees,0.5900,0.7500,0.8100,0.31
MainModel (Optimized),0.5600,0.7500,0.8200,1.72
voting_ensemble,0.5700,0.7300,0.8300,3.01
```

#### **Hyperparameter Optimization Results** (sample from actual run)
```json
{
  "best_parameters": {
    "n_bags": 5,
    "dropout_strategy": "linear", 
    "epochs": 10,
    "batch_size": 256,
    "random_state": 49
  },
  "best_mae_score": 0.750,
  "optimization_trials": 12,
  "improvement_over_default": 0.090
}
```

#### **Baseline Model Summary** (sample from actual run)
```json
{
  "total_models": 28,
  "successful_models": 28,
  "failed_models": 0,
  "best_model": "voting_ensemble",
  "best_accuracy": 0.59,
  "best_mae": 0.73,
  "fastest_model": "combined_knn",
  "fastest_time": 0.0005,
  "total_time": 36.99
}
```

### **Key Findings Example** (from actual experiment)

**Top Performing Models**:
1. **metadata_lr** (Logistic Regression on Metadata): 59.0% accuracy, 0.79 MAE
2. **extra_trees** (Extra Trees Ensemble): 59.0% accuracy, 0.75 MAE  
3. **voting_ensemble** (Voting Ensemble): 57.0% accuracy, 0.73 MAE
4. **MainModel (Optimized)**: 56.0% accuracy, 0.75 MAE

**Performance Insights**:
- **Metadata Features**: Surprisingly effective for rating prediction
- **Ensemble Methods**: Consistent top performance across multiple algorithms
- **MainModel Performance**: Competitive with baselines, showing potential for improvement
- **Training Efficiency**: Models range from 0.01s to 10.4s training time

### **Statistical Analysis Framework**

#### **Cross-Validation Results** (sample format)
```json
{
  "RandomForest_Text": {
    "mean_accuracy": 0.550,
    "std_accuracy": 0.026,
    "confidence_interval_95": [0.515, 0.585]
  },
  "MainModel": {
    "mean_accuracy": 0.550, 
    "std_accuracy": 0.000,
    "confidence_interval_95": [0.550, 0.550]
  }
}
```

#### **Ablation Study Results** (sample format)
```json
{
  "modality_ablation": {
    "text_only": {"accuracy": 0.590, "mae": 0.780},
    "metadata_only": {"accuracy": 0.590, "mae": 0.790},
    "both_modalities": {"accuracy": 0.600, "mae": 0.750}
  },
  "fusion_strategy_ablation": {
    "early_fusion": {"accuracy": 0.590, "mae": 0.780},
    "late_fusion": {"accuracy": 0.630, "mae": 0.730},
    "weighted_fusion": {"accuracy": 0.600, "mae": 0.750}
  }
}
```

---

## ğŸš€ Experiment Reproduction Guide

### **Two-Mode Execution Framework**

#### **Mode 1: Quick Run Mode (Development & Debugging)**
**Purpose**: Fast feedback for development, debugging, and preliminary results

**Configuration**:
- **Dataset Size**: 500 training samples, 100 test samples (subset)
- **Test Runs**: 1 single run with fixed seed (42)
- **Hyperparameter Trials**: 12 combinations (reduced search space)
- **Execution Time**: ~2 minutes
- **Pipeline Coverage**: Full 6-phase pipeline with reduced scope

#### **Mode 2: Full Run Mode (Production & Publication)**
**Purpose**: Comprehensive evaluation for publication-quality results

**Configuration**:
- **Dataset Size**: Full dataset (200,000 train, 50,000 test)
- **Test Runs**: 5 runs with different seeds (42, 123, 456, 789, 999)
- **Hyperparameter Trials**: 52 combinations (full search space)  
- **Execution Time**: ~2-6 hours
- **Pipeline Coverage**: Complete 6-phase pipeline with full scope

### **Execution Methods**

#### **Method 1: Interactive Command Line**
```bash
# Launch interactive mode selector
python3 run_main.py

# Follow prompts:
# 1. ğŸš€ Quick Run Mode (FAST)
# 2. ğŸ¯ Full Run Mode (COMPREHENSIVE)
# Enter choice (1-2) [default: 1]:
```

#### **Expected Output Example** (from actual run):
```
ğŸ›’ Amazon Reviews 5-Class Rating Prediction Framework
================================================================================
ğŸ“Š Dataset: Amazon Product Reviews
ğŸ¯ Task: 5-class rating prediction (1â˜… to 5â˜…)

ğŸš€ STARTING AMAZON REVIEWS FULL EXPERIMENT
======================================================================

ğŸ“ PHASE 1: DATA PREPARATION
âœ… Dataset loaded: 500 train, 100 test samples
âœ… Text features: 1,000 dimensions, Metadata: 14 dimensions

ğŸ“Š PHASE 2: BASELINE MODEL EVALUATION  
âœ… 28 models tested across 5 categories
ğŸ† Best baseline: voting_ensemble (MAE: 0.730)

ğŸ§  PHASE 3: MAINMODEL EVALUATION
âœ… Hyperparameter search: 12 trials completed
ğŸ† Best MAE: 0.750 (n_bags=5, dropout=linear, epochs=10)

ğŸ”¬ PHASE 4: ADVANCED ANALYSIS
âœ… Cross-validation: 5-fold completed
âœ… Robustness testing: 4 categories completed
âœ… Interpretability: Feature importance analyzed
âœ… Ablation studies: 5 types completed

ğŸ“‹ PHASE 5: COMPREHENSIVE REPORTING
âœ… Performance ranking generated
âœ… Statistical analysis completed

ğŸ’¾ PHASE 6: RESULTS MANAGEMENT
âœ… Results saved to organized structure
âœ… Publication materials exported

ğŸ‰ EXPERIMENT COMPLETED SUCCESSFULLY!
ğŸ“Š Total runtime: 112.5 seconds (1.9 minutes)
ğŸ¥‡ Best baseline: voting_ensemble (Accuracy: 0.5900)
ğŸ§  MainModel (Optimized): Accuracy 0.5600
ğŸ“ Results saved to: results/run_20250815_180252
```

### **Result Analysis & Interpretation**

#### **Performance Metrics Understanding**
- **Accuracy**: Exact rating match (e.g., 0.59 = 59% exact matches)
- **Star MAE**: Mean Absolute Error in star ratings (lower is better)
- **Â±1 Star Accuracy**: Percentage within 1 star of true rating (ordinal tolerance)
- **Training Time**: Model training duration in seconds

#### **Model Comparison Guidelines**
1. **Primary Metric**: Star MAE (optimized for rating prediction)
2. **Secondary Metrics**: Accuracy and Â±1 Star Accuracy
3. **Efficiency Consideration**: Training time for deployment decisions
4. **Robustness Assessment**: Performance under adverse conditions

#### **Statistical Significance Validation**
- **Cross-Validation**: 5-fold CV with confidence intervals
- **Multi-Seed Testing**: Multiple random initializations (Full Mode)
- **Significance Testing**: Paired t-tests between models
- **Effect Size**: Practical significance beyond statistical significance

### **Troubleshooting & Support**

**Common Issues**:
1. **Memory Errors**: Use Quick Mode for large datasets
2. **Import Errors**: Ensure MainModel is in Python path
3. **Data Missing**: Verify preprocessed data location
4. **Slow Performance**: Quick Mode for development, Full Mode for publication

**Performance Optimization**:
- Quick Mode: ~2 minutes (development)
- Full Mode: ~2-6 hours (publication)
- Monitor memory usage on large datasets
- Use multiprocessing for parallel evaluation

**Result Validation**:
- Check experiment manifest for completeness
- Verify statistical significance in cross-validation
- Compare against established baselines
- Validate reproducibility with fixed seeds

---

## ğŸ“Š Expected Research Outcomes

### **Scientific Contributions**
1. **Novel Architecture Validation**: Empirical evidence for modality-aware ensemble effectiveness
2. **Comprehensive Benchmarking**: Extensive baseline comparison across 30+ models
3. **Statistical Rigor**: Confidence intervals and significance testing
4. **Robustness Analysis**: Performance under real-world conditions
5. **Interpretability Insights**: Understanding of multimodal decision processes

### **Practical Applications**
- **E-commerce**: Review sentiment analysis and rating prediction
- **Recommendation Systems**: User preference modeling
- **Quality Assessment**: Product and service evaluation automation
- **Multimodal Learning**: General framework for text + metadata tasks

---

This comprehensive experiment framework provides rigorous scientific evaluation of the Modality-Aware Adaptive Bagging Ensemble on Amazon Reviews rating prediction, with complete reproducibility, statistical validation, and publication-ready outputs based on actual experimental results.
